# ğŸ¡ PrediÃ§Ã£o de PreÃ§os de Casas com Random Forest e Decision Tree

## ğŸ“Œ DescriÃ§Ã£o
Este projeto aplica **Machine Learning (RegressÃ£o)** para prever preÃ§os de imÃ³veis utilizando o dataset `train.csv`.  
O objetivo Ã© comparar a performance entre o **Random Forest Regressor** e o **Decision Tree Regressor** com diferentes nÃºmeros de folhas (`max_leaf_nodes`).  

AlÃ©m de treinar os modelos, o notebook tambÃ©m apresenta uma **anÃ¡lise visual (linha e barras)** do **Erro MÃ©dio Absoluto (MAE)** em funÃ§Ã£o do nÃºmero de folhas, explorando conceitos de **underfitting e overfitting**.

---

## ğŸ¯ Objetivos
- Entender o funcionamento de **Ã¡rvores de decisÃ£o** e **ensembles (Random Forest)**.  
- Avaliar o impacto do nÃºmero de folhas na performance de uma Decision Tree.  
- Comparar a capacidade de generalizaÃ§Ã£o entre modelos simples e ensembles.  
- Praticar o uso de **Pandas** e **Scikit-learn** para regressÃ£o supervisionada.  

---

## ğŸ›  Tecnologias Utilizadas
- Python 3  
- Pandas  
- Scikit-learn  
- Matplotlib  

---

## ğŸ“‚ Estrutura
- `random_forest_regressor.ipynb` â†’ Notebook principal do projeto.  
- `train.csv` â†’ Dataset de preÃ§os de casas (House Prices).  

---

## ğŸš€ Como Executar
1. Clone este repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/nome-do-repo.git
   cd nome-do-repo
   ```
2. Instale as dependÃªncias:
   ```bash
   pip install pandas scikit-learn matplotlib
   ```
3. Abra o notebook no Jupyter ou PyCharm e execute cÃ©lula por cÃ©lula:
   ```bash
   jupyter notebook random_forest_regressor.ipynb
   ```

---

## ğŸ“Š Resultados
- **Random Forest** apresentou bom desempenho, com menor MAE em relaÃ§Ã£o a Ã¡rvores de decisÃ£o muito simples ou muito complexas.  
- A **Decision Tree com 50 folhas** apresentou o **melhor resultado entre as Ã¡rvores**, superando configuraÃ§Ãµes com 5, 500 e 5000 folhas.  

### VisualizaÃ§Ã£o (exemplo de saÃ­da):
#### ğŸ“ˆ GrÃ¡fico de Linha
Mostrando o comportamento do MAE em funÃ§Ã£o do nÃºmero de folhas:  
- Poucas folhas â†’ **underfitting** (erro alto).  
- Muitas folhas â†’ **overfitting** (erro volta a subir).  
- NÃºmero intermediÃ¡rio (~50) â†’ **melhor equilÃ­brio**.  

---

## ğŸ“ˆ Aprendizados
- **Underfitting:** modelos simples demais nÃ£o capturam padrÃµes (5 folhas).  
- **Overfitting:** modelos muito complexos memorizam os dados (5000 folhas).  
- **Trade-off viÃ©s x variÃ¢ncia:** o melhor ponto estÃ¡ no meio (~50 folhas).  
- O **Random Forest** tende a generalizar melhor, mas uma Decision Tree bem configurada pode chegar prÃ³ximo.  


